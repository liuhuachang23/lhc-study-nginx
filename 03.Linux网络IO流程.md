# Linux网络IO流程

1. 服务器网卡收到客户端请求，如果网卡没有开启混合模式，那么当数据包目的mac不是自身时会将数据包丢弃

> 网卡接收到数据后接下来要考虑如何将数据发送到内存中，因为所有的硬件或OS都是从内存中读取数据
>
> - 硬中断：在早前，网卡接收到数据后会产生一个CPU的中断，通知CPU网卡中有新的数据，然后CPU将数据从网卡拷贝到内存中，这个过程中CPU会产生一个硬中断。在正常情况下，CPU在处理一些进程，如果此时发生中断则代表着CPU要放下正在处理的进程而优先先处理中断进程，这会对整体的硬件性能造成影响
> - DMA：为了避免由于硬中断对设备性能造成的影响，近代的网卡设备上都会带有DMA机制，DMA表示，当网卡接收到新的数据时不需要再去CPU产生中断，而是自动将数据拷贝至内存，网卡将数据拷贝至内存后再去找CPU产生中断，而网卡本身是有一个数据缓存区的，所以通过DMA的方式能够极大的提高CPU的工作效率

2. 网卡通过DMA机制将数据从网卡拷贝至内存，并产生一个中断
3. CPU通过中断表查询产生中断的来源，然后去找相对应的网卡驱动

> 通过DMA机制将数据从网卡保存到内存中的数据其本质上还是比较原始的报文，而原始报文无法被Linux的内核识别，所以需要通过网卡驱动将原始数据转换为Linux内核的网络协议簇（也就是Linux内核能够识别的报文）
>
> - 消息队列`input_pkt_queue`：为了防止因为数据过多，导致Linux网络协议簇无法及时处理所有数据而丢弃数据的问题，网卡驱动将原始报文转换后不会直接交给Linux的网络协议簇，而是将转换后的数据放到消息队列中，Linux要读时直接从队列中读取。如果队列中的数据满了，那么数据也会被丢弃掉，可以通过调整系统参数`net.core.netdev_max_backlog`来调整队列大小
> - wireshark：抓包的过程实际上也是拷贝消息队列中的数据进行分析

4. Linux从消息队列中读取报文并从底层开始拆包
5. 通过链路层协议查看数据包中的目的mac是否为自身mac（`iptables`的`netfilter`表就是工作在链路层）

> 如果dmac是本机，则对数据做接收，如果不是本机，那么也可以通过`iptables`对dmac做修改，无论出于什么原因，还是接收了此数据

6. 将数据包交由网络层协议查看IP地址，如果DIP是本机则接收数据，如果不是本机则查看内核是否开启了`ip_forward`，如果内核没有开启`ip_forward`且DIP不是本机，则丢弃报文
7. 传输层协议通过数据包中的IP和端口来查找相应的`socket`，如果本机上有服务再监听相应的端口，则将数据放到`socket`的队列中（这里涉及到连接的建立，半连接与全连接队列），然后由内核通知相关的应用进行数据读取

> - 半连接：TCP的三次握手，客户端向服务端发送一个`SYN`请求，服务端收到后会将此请求放在一个有`SYN`的标志位的半连接的队列中
> - 全连接：TCP三次握手完成后，服务端收到了客户端发送回来的`ACK`报文后，将客户端请求从半连接队列中取出变为全连接队列

8. 应用层服务将请求报文打开后，需要将客户请求的资源封装打包好再发送到客户端，但应用服务本身是没有权限使用主机上的资源的，只能通过内核获取所需要的资源

> 应用服务请求数据的时候发生了系统I/O调用，而系统调用又分4种：同步、异步、阻塞、非阻塞
>
> - 阻塞：应用服务向内核发起操作请求后，这过程中应用服务没有任何其他动作，一直等待内核回复后才恢复工作。没有任何动作表示堵塞，也是同步
> - 非阻塞：应用服务向内核发起操作请求后，再等待过程中不断向内核发起询问资源是否调用完成，如果资源没有调用完成，内核会向应用服务回复一个错误信息。不断询问表示非阻塞
> - 同步：应用程序在向内核请求资源时一直等待，没有其他动作
> - 异步：应用程序在向内核请求资源的同时继续保持提供服务，处理其他任务

9. 内核读取完资源后会将其放在内核缓存区中处理，处理完后再拷贝到应用服务的缓存区中，应用层获取数据后再对数据进行重新封包，并发送到网卡发送缓存区

# Linux网络I/O模型

网络I/O模型属于nginx服务请求系统调用(`system call`)的过程描述，当应用接收到客户请求时，如何针对请求进行处理就涉及到Linux的网络I/O模型。当用户空间的应用程序需要读写硬盘上的数据时，会通过系统调用的方式通知内核处理，此时进入内核空间，内核将数据从硬盘读取到内核缓冲区，然后进行数据的读写处理，数据准备完毕后，将其拷贝到应用程序的缓冲区，此时进入用户空间由应用程序再对数据进行处理。这里会涉及到两个阶段

- **等待数据准备就绪(Waiting for the data to be ready)**
- **等数据从内核缓冲区拷贝到进程缓冲区(Copying the data from the kernel to the process)**

因为这两个阶段，linux产生了5中网络模式：

- **阻塞式I/O模型(Blocking IO model)**
- **非阻塞式I/O模型(noblocking IO model)**
- **I/O复用式I/O模型(IO multiplexing model)**
- **信号驱动式I/O模型(signal-driven IO model)**
- **异步I/O式I/O模型(asynchronous IO model)**

1. 阻塞式I/O模型(Blocking IO model)

                        `application`                                                        `kernel`

                     `read` --------------------`system call`----------> `no data ready` ---------->

         process     |                                                         ⬇                   | `wait for data`

       blocks in     |                                                     `data ready copy data`--

          a call     |                           `return OK`                   ⬇                   | copy data from

         to read      <--------------------------------------------------- `copy complete` -----  kernel to user

在linux中，默认情况下的所有I/O操作都是`blocking`，阻塞模型下，应用程序在等待内核处理数据的两个阶段的过程中没有任何操作，无论是等待数据准备就绪或是将数据从内核拷贝到进程缓冲区，这其中都需要一个过程，内核将数据准备好了并拷贝至用户空间的缓存区后，返回一个结果，此时用户进程才解除`block`状态

2. 非阻塞式I/O模型(noblocking IO model)

                              `application`                                                        `kernel`

                     ---------------`read` -----------------------------------> `no data ready` ------->

         process     |           `EWOULDBLOCK` <------> `system call`                  ⬇               | `wait for data`

       blocks in     |           `EWOULDBLOCK` <------> `system call`           `data ready copy data`--

          a call     |                                    `return OK`                  ⬇               | copy data from

         to read      <----------------------------------------------------------- `copy complete` -----  kernel to user

在linux中，可以通过设置socket使其变为`non-blocking`，socket设置为nonblock后，当应用程序请求的I/O操作无法完成时，不将进程睡眠，而是返回一个错误码(`EWOULDBLOCK`)，应用收到错误码时可以进行其他动作，且应用会在等待过程中多次询问内核数据是否已经处理完成，但这仅限于等待数据准备就绪阶段，数据准备完成后应用必须处于阻塞状态，等待内核将数据拷贝至应用缓存区

3. I/O多路复用(IO multiplexing)

                       `application`                                                            `kernel`   
                   

 process blocks|---------`select` -------------`system call`------------> `no data ready` ---------->|

waiting for one|                                                                 ⬇                   |`wait for data`

   of many fds | <--------------------`return readable`------------------ `data ready`---------------|

               | `read` ----------------------- `system call` -----------> `copy data` --------------| 
    
     process   |                                                               ⬇                     |copy data from
    
      blocks   |`process data`<------------ `return OK` -------------- `copy complete` --------------| kernel to user 

多路复用模型比较常见的有`select,poll,epoll`，也称这种I/O方式为事件驱动去模型`even driven IO`。多路复用模型与堵塞模型是非常相似的，多路复用引入了一个中间人`select`，所有的进程需要进行系统调用时不再直接找内核，而是找`select`，通过`select`进行系统调用，在内核返回数据的两个阶段中，应用都处于堵塞状态。

多路复用的优势是在高并发的情况下提升性能，每个应用程序需要读取资源时都会调用`select`，由`select`全权代理再将请求发送给`kernel`，同时`kernel`会监视所有`select`负责的`socket`，当任何一个`socket`中的数据准备完成后，`select`就会返回。每一个`select`可以处理多个应用进程，而`kernel`也只需要再对`select`响应即可

>多路复用模型再并发请求不多的情况下，性能不如阻塞模型。对于`select/poll/epoll`可以根据调用时指定的参数不同，而决定在内核返回数据的两个阶段中是否堵塞或不堵塞

4. 信号驱动式I/O模型(signal-driven IO model)

首先允许`socket`进行信号驱动I/O，并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个`sigio`信号，可以再信号处理函数中调用I/O操作函数处理数据

5. 异步I/O式I/O模型(asynchronous IO model)

                              `application`                                                        `kernel`

                     |`aio_read` --------------`system call`---------------> `no data ready` ------------>

         process     |                        `return`                              ⬇                    | `wait for data`

       blocks in     |                                                     `data ready copy data`--------

          a call     |                   `return OK`                                ⬇                    | copy data from

         to read      <-----------------`deliver signal`----------------- `copy complete` -----  kernel to user

用户进程发起`aio_read`调用后可以继续其他工作，从`kernel`的角度看，当它发现一个异步I/O后，首先它会立刻返回，所以不会对应用进程产生任何`block`，然后`kerbel`会等待数据准备完成并将其拷贝到应用缓存区，两个阶段都完成后再向应用进程发送一个`signal`信号
